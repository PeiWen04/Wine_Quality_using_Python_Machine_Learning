# -*- coding: utf-8 -*-
"""Copy of CPC152 Assg 2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1i2U-BitQeZEE4VdeSUto09WOYWev9eu5

#Import the libraries and data
"""

# importing library
import numpy as np
import matplotlib.pyplot as plt
import numpy.random as npr
import pandas as pd

# Read and print the dataset
df = pd.read_csv("Data set 3 (99 KB) - winequality.csv")
df

# Get summary of numerical variables
# describe() function would provide count, mean, standard deviation (std), min, quartiles and max in its output
df.describe()

"""# Data Cleaning and Data Processing"""

# Check the number of null in the dataset
df.apply(lambda x: sum(x.isnull()),axis=0)

df.duplicated().sum()

df.drop_duplicates(inplace = True)

df.dtypes

df.info()

"""Group the wine into 2 classes : Quality >= 6 will be Good Wine Quality ,Quality< 6 will be Bad Wine Quality"""

# Group the target variable into two classes (1=good, 0=bad)
df['quality_binary'] = df['quality'].apply(lambda x: 1 if x >= 6 else 0)

# Drop the 'quality' column
df = df.drop(columns=['quality'])

# Display the updated DataFrame
df.head()

import pandas as pd
import matplotlib.pyplot as plt

# Plot histograms for each feature
df.hist(bins=15, figsize=(15, 10))
plt.tight_layout()
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt

# Define the subset of columns for the pairplot
columns_to_plot = ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides',
                   'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol']

# Pairplot to see relationships between the specified variables
sns.pairplot(df[columns_to_plot])

# Display the plot
plt.show()

import seaborn as sns

# Plot a correlation heatmap
plt.figure(figsize=(12, 8))
sns.heatmap(df.corr(), annot=True, cmap='coolwarm', vmin=-1, vmax=1)
plt.show()

"""#Splitting Data"""

from sklearn.model_selection import train_test_split

X = df.drop('quality_binary',axis=1)
y = df['quality_binary']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

print(f"No. of training samples: {X_train.shape[0]}")
print(f"No. of testing samples: {X_test.shape[0]}")

from sklearn.preprocessing import StandardScaler
scaler=StandardScaler()
x_train=scaler.fit_transform(X_train)
x_test=scaler.fit_transform(X_test)
print(x_train)
print(x_test)

"""#Feature *Selection*"""

import pandas as pd
from sklearn.ensemble import ExtraTreesClassifier
import matplotlib.pyplot as plt

features = ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides',
            'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol']
X = df[features]
y = df['quality_binary']

# ExtraTreesClassifier to get feature importances
model = ExtraTreesClassifier()
model.fit(X, y)

# Create a DataFrame for better visualization of feature importances
feature_importances = pd.DataFrame({
    'Features': features,
    'Importance': model.feature_importances_
})

# Sort the features by importance
feature_importances = feature_importances.sort_values(by='Importance', ascending=False)

print("Feature importances DataFrame:")
print(feature_importances)

# Plotting feature importances
plt.figure(figsize=(10, 6))
feature_importances.set_index('Features')['Importance'].nlargest(10).plot(kind='barh')
plt.xlabel('Importance')
plt.ylabel('Features')
plt.title('Top 10 Feature Importances')
plt.gca().invert_yaxis()  # Invert y axis to have the most important feature at the top
plt.show()

"""From feature selection, there are 5 important features:
alcohol
sulphates
volatile acidity
citric acid
total sulfur dioxide

#KNN
"""

import matplotlib.pyplot as plt
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

# Define the top features based on importance
top_features = ['alcohol', 'sulphates', 'volatile acidity', 'total sulfur dioxide', 'density']

# Define a list of k values to test
k_values = [3, 5, 7, 9]

# Define a list of numbers of features to test
num_features_list = list(range(1, 6))

# Dictionary to store accuracy results for each k
accuracy_results = {k: [] for k in k_values}

# Iterate over different numbers of features (1 to 5)
for num_features in num_features_list:
    selected_features = top_features[:num_features]
    x_train_subset = X_train[selected_features]
    x_test_subset = X_test[selected_features]

    print(f"\nTesting KNN models with {num_features} features: {selected_features}")

    # List to store accuracy values for the current num_features
    accuracy_values = []

    # KNN with different k values
    for k in k_values:
        knn_model = KNeighborsClassifier(n_neighbors=k)
        knn_model.fit(x_train_subset, y_train)
        y_pred = knn_model.predict(x_test_subset)
        accuracy = accuracy_score(y_test, y_pred)
        accuracy_values.append(accuracy)
        print(f"KNN with k={k}: Accuracy={accuracy:.4f}")

    # Store accuracy values for the current num_features
    for idx, k in enumerate(k_values):
        accuracy_results[k].append(accuracy_values[idx])

# Plotting the results
plt.figure(figsize=(10, 6))

for k in k_values:
    plt.plot(num_features_list, accuracy_results[k], marker='o', label=f'k={k}')

plt.title('Accuracy of KNN Models with Different Numbers of Features and k Values')
plt.xlabel('Number of Features')
plt.ylabel('Accuracy')
plt.xticks(num_features_list)
plt.legend(title='k Values')
plt.grid(True)
plt.tight_layout()
plt.show()

"""Define the best KNN model"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

# Define the top features based on importance
top_features = ['alcohol', 'sulphates', 'volatile acidity', 'total sulfur dioxide', 'density']


# Standardize the data
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Define the best KNN model
# Example: Assume k=5 is determined as the best
best_model = KNeighborsClassifier(n_neighbors=9)

# Fit the best model on the standardized training data
best_model.fit(X_train_scaled, y_train)

# Make predictions on the standardized test set
y_pred = best_model.predict(X_test_scaled)

# Calculate evaluation metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
confusion_mat = confusion_matrix(y_test, y_pred)

# Print the evaluation metrics
print("Evaluation Metrics for the Best KNN Model:")
print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)
print("F1 Score:", f1)
print("Confusion Matrix:\n", confusion_mat)

"""# SVM"""

from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt

# Define the top features based on importance
top_features = ['alcohol', 'sulphates', 'volatile acidity', 'density', 'total sulfur dioxide']

# Define a list of C values to test (regularization parameter) for linear and RBF kernels
C_values = [0.1, 1, 10]

# Define a list of degrees for polynomial kernel
degrees = [2, 3, 4]

# Define a list of gamma values for RBF kernel
gamma_values = [0.1, 1, 10]

# Initialize a dictionary to store accuracy results for each kernel type and their parameters
accuracy_results = {
    'linear': {C: [] for C in C_values},
    'poly': {degree: [] for degree in degrees},
    'rbf': {gamma: {C: [] for C in C_values} for gamma in gamma_values}
}

# Iterate over different numbers of features (2, 3, 4, 5)
num_features_list = list(range(2, 6))
for num_features in num_features_list:
    selected_features = top_features[:num_features]
    x_train_subset = X_train[selected_features]
    x_test_subset = X_test[selected_features]

    print(f"\nTesting SVM models with {num_features} features: {selected_features}")

    # Linear SVM with different C values
    for C in C_values:
        linear_model = SVC(kernel='linear', C=C)
        linear_model.fit(x_train_subset, y_train)
        y_pred = linear_model.predict(x_test_subset)
        accuracy = accuracy_score(y_test, y_pred)
        accuracy_results['linear'][C].append(accuracy)
        print(f"Linear SVM with C={C}: Accuracy={accuracy:.4f}")

    # Polynomial SVM with different degrees
    for degree in degrees:
        poly_model = SVC(kernel='poly', degree=degree, gamma='scale', coef0=1, random_state=42)
        poly_model.fit(x_train_subset, y_train)
        y_pred = poly_model.predict(x_test_subset)
        accuracy = accuracy_score(y_test, y_pred)
        accuracy_results['poly'][degree].append(accuracy)
        print(f"Polynomial SVM with degree={degree}: Accuracy={accuracy:.4f}")

    # RBF SVM with different gamma and C values
    for gamma in gamma_values:
        for C in C_values:
            rbf_model = SVC(kernel='rbf', gamma=gamma, C=C)
            rbf_model.fit(x_train_subset, y_train)
            y_pred = rbf_model.predict(x_test_subset)
            accuracy = accuracy_score(y_test, y_pred)
            accuracy_results['rbf'][gamma][C].append(accuracy)
            print(f"RBF SVM with gamma={gamma}, C={C}: Accuracy={accuracy:.4f}")

# Plotting the results
plt.figure(figsize=(15, 12))

# Plot Linear SVM results
plt.subplot(2, 2, 1)
for C in C_values:
    plt.plot(num_features_list, accuracy_results['linear'][C], marker='o', label=f'C={C}')
plt.title('Linear SVM Accuracy')
plt.xlabel('Number of Features')
plt.ylabel('Accuracy')
plt.xticks(num_features_list)
plt.legend(title='C Values')
plt.grid(True)

# Plot Polynomial SVM results
plt.subplot(2, 2, 2)
for degree in degrees:
    plt.plot(num_features_list, accuracy_results['poly'][degree], marker='o', label=f'degree={degree}')
plt.title('Polynomial SVM Accuracy')
plt.xlabel('Number of Features')
plt.ylabel('Accuracy')
plt.xticks(num_features_list)
plt.legend(title='Degree Values')
plt.grid(True)

# Plot RBF SVM results
plt.subplot(2, 2, 3)
for gamma in gamma_values:
    for C in C_values:
        plt.plot(num_features_list, accuracy_results['rbf'][gamma][C], marker='o', label=f'gamma={gamma}, C={C}')
plt.title('RBF SVM Accuracy')
plt.xlabel('Number of Features')
plt.ylabel('Accuracy')
plt.xticks(num_features_list)
plt.legend(title='Gamma, C Values')
plt.grid(True)

plt.tight_layout()
plt.show()

"""Define the best SVM model"""

from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
from sklearn.preprocessing import StandardScaler

# Define the best SVM model
best_model = SVC()

# Fit the best model on the standardized training data
best_model.fit(X_train_scaled, y_train)

# Make predictions on the standardized test set
y_pred = best_model.predict(X_test_scaled)

# Calculate evaluation metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
confusion_mat = confusion_matrix(y_test, y_pred)

# Print the evaluation metrics
print("Evaluation Metrics for the Best SVM Model:")
print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)
print("F1 Score:", f1)
print("Confusion Matrix:\n", confusion_mat)

"""#Decision Tree"""

from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt

# Define the top features based on importance
top_features = ['alcohol', 'sulphates', 'volatile acidity', 'density', 'total sulfur dioxide']

# Define a list of max_depth values to test (parameter for Decision Tree)
max_depth_values = [3, 5, 7, 10]

# Initialize empty dictionary to store accuracy results
accuracy_results = {depth: [] for depth in max_depth_values}

# Iterate over different numbers of features (2, 3, 4, 5)
for num_features in range(2, 6):
    selected_features = top_features[:num_features]
    x_train_subset = X_train[selected_features]
    x_test_subset = X_test[selected_features]

    print(f"\nTesting Decision Tree models with {num_features} features: {selected_features}")

    # Decision Tree with different max_depth values
    for max_depth in max_depth_values:
        dt_model = DecisionTreeClassifier(max_depth=max_depth, random_state=42)
        dt_model.fit(x_train_subset, y_train)
        y_pred = dt_model.predict(x_test_subset)
        accuracy = accuracy_score(y_test, y_pred)
        accuracy_results[max_depth].append(accuracy)  # Store accuracy for this max_depth
        print(f"Decision Tree with max_depth={max_depth}: Accuracy={accuracy:.4f}")

# Plotting the results
plt.figure(figsize=(10, 6))

for depth in max_depth_values:
    plt.plot(range(2, 6), accuracy_results[depth], marker='o', label=f'max_depth={depth}')

plt.title('Accuracy of Decision Tree Models with Different Numbers of Features and max_depth Values')
plt.xlabel('Number of Features')
plt.ylabel('Accuracy')
plt.xticks(range(2, 6))
plt.legend(title='max_depth Values')
plt.grid(True)
plt.tight_layout()
plt.show()

"""Define the best Decision Tree model"""

from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

# Define the best Decision Tree model (adjust parameters as needed)
best_model = DecisionTreeClassifier(max_depth=5, min_samples_split=2, random_state=42)

# Fit the best model on the training data
best_model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = best_model.predict(X_test)

# Calculate evaluation metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
confusion_mat = confusion_matrix(y_test, y_pred)

# Print the evaluation metrics
print("Evaluation Metrics for the Best Decision Tree Model:")
print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)
print("F1 Score:", f1)
print("Confusion Matrix:\n", confusion_mat)